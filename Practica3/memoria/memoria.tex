\documentclass{uc3mpracticas}

\usepackage{helvet}
\usepackage{multicol}
\renewcommand{\familydefault}{\sfdefault}
\usepackage{changepage}
\usepackage{geometry}
\usepackage{caption}
\usepackage{xcolor,colortbl}
\usepackage{makecell}
\usepackage{mathtools}

\usepackage{amsfonts}

\definecolor{Gray}{gray}{0.85}
\definecolor{LightCyan}{rgb}{0.88,1,1}
\definecolor{LightGreen}{rgb}{0.29,1,0.39}

\newcolumntype{g}{>{\columncolor{Gray}}l}
\newcolumntype{b}{>{\columncolor{LightCyan}}c}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%                   Plantilla Prácticas UC3M                               %%%
%%%                Universidad Carlos III de Madrid                          %%%
%%%                   Alejandro Valverde Mahou                               %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%Permitir cabeceras y pie de páginas personalizados
\pagestyle{fancy}

%Path por defecto de las imágenes
\graphicspath{ {./images/} }

%Declarar formato de encabezado y pie de página de las páginas del documento
\fancypagestyle{doc}{
  %Cabecera
  \headerpr[1]{Optimización combinatoria}{}{Teoría Avanzada de la Computación}
  %Pie de Página
  \footerpr{}{\textbf{UC3M}}{{\thepage} de \pageref{LastPage}}
}

%Declarar formato de encabezado y pie del título e indice
\fancypagestyle{titu}{%
  %Cabecera
  \headerpr{}{}{}
  %Pie de Página
  \footerpr{}{}{}
}


\appto\frontmatter{\pagestyle{titu}}
\appto\mainmatter{\pagestyle{doc}}


\begin{document}
  %Comienzo formato título
  \frontmatter


  %Portada 1 (Centrado todo)
  \centeredtitle{Images/LogoUC3M.png}{Grado en Ingeniería Informática}{Curso 2020/2021}{Teoría Avanzada de la Computación}{Optimización combinatoria}{}

  \vspace{55mm}

  \authors{Iván Miguélez García}{100383387}{Alba Reinders Sánchez}{100383444}{Alejandro Valverde Mahou}{100383383}{}{}

  \newpage

  %Índice
  \tableofcontents

\newpage

  %Comienzo formato documento general
  \mainmatter

  \section{Introducción}

  En esta práctica se plantea resolver el TSP (\textit{Travelling Salesman Problem}), también conocido como problema del viajante, a través de dos algoritmos distintos. El primer algortimo es DFS, basado en búsqueda en profundidad, mientras que el segundo algoritmo utiliza un algoritmo \textit{greedy}, para aplicar búsqueda local y tratar de mejorar la solución obtenida con el operador 2-opt. Para desarrollar ambos algoritmos, se va a utilizar \textit{C++}, ya que ofrece mayor rendimiento que otros lenguajes de programación.

  \vspace{2mm}

  El objetivo es desarrollar y analizar estos algoritmos para resolver el problema de optimización combinatoria TSP. Éste es un problema que plantea qué camino tomar dadas una serie de ciudades conectadas, empezando y terminando en la misma ciudad, de forma que la solución tenga la menor distancia posible. La solución de este problema consiste en encontrar el ciclo Hamiltoniano que tenga el menor tamaño. Encontrarlo no es una tarea trivial, dado que el número de caminos posibles crece factorialmente al añadir una ciudad más.

  \vspace{2mm}

  El TSP se considera como un problema NP-completo, y es uno de los algoritmos más estudiados, para el cuál existen numerosos métodos de resolución y heurísticas. Para esta práctica se va a realizar un acercamiento por fuerza bruta a través del algoritmo de profundidad, un acercamiento ligeramente más sofisticado con una poda sobre la fuerza bruta y una nueva implementeación con búsqueda local.

  \vspace{2mm}

  La memoria se divide en dos apartados principales, uno para el algoritmo DFS y otro para el algoritmo de búsqueda local. En cada apartado se estudia el coste computacional de cada algoritmo mediante un estudio analítico y empírico del mismo. Además, en el algoritmo de DFS se analizan dos variantes, uno básico y uno al que se le aplica poda.

  \vspace{5mm}

  Para permitir la replicabilidad de los resultados, así como para poder comprobar los resultados obtenidos, se va a hacer uso de un cuaderno de \textit{Python} en la herramiento de \textit{Google Colab}, donde se ejecutarán los distintos algoritmos y se generarán las distintas gráficas comparativas.


\clearpage

  \section{Algoritmo de búsqueda iterativa}

  El algoritmo que se plantea es DFS (\textit{Depth First Search}). Este algoritmo consiste en expandir desde el nodo raíz, que representa la posición inicial, todos sus nodos hijos. A continuación, se expanden todos los nodos del primer hijo. Esto se repite por el mismo camino hasta que termina y vuelve al origen. Una vez termina, va iterando hacia atrás y realiza el mismo proceso con todos sus nodos hermanos.

  \vspace{2mm}

  Para la implementación de este algoritmo se ha hecho uso de una pila de nodos expandidos, y cada vez que un nodo generaba nuevos hijos, se añadían a la pila. Cada camino se completa cuando no es capaz de generar más hijos porque ya ha visitado todos los nodos.

  \vspace{2mm}

  Este algoritmo, al aplicarse en el problema del TSP, genera todos los ciclos Hamiltonianos posibles comenzando en un mismo punto y, por tanto, es completo. También se puede asegurar su optimalidad ya que, al generar todos los caminos, y ser todos del mismo tamaño, tan solo hay que almacenar el camino que genera mejores resultados.

  \vspace{2mm}

  Al tener que expandir todos los nodos, el tiempo de computo aumenta considerablemente según el tamaño del problema crece, es decir, cuando se añaden más ciudades el número de nodos a generar crece muy rápido. Esto hace que se busque algún método para reducir el número de nodos a generar. Esta mejora consiste en realizar una poda del árbol de búsqueda cuando el coste acumulado del camino es mayor que el costed del mejor camino ya encontrado.



  \subsection{Coste Computacional DFS básico}

  \subsubsection{Estudio Analítico}

  Usando $n$ como el número de ciudades del problema, el número de ciclos Hamiltonianos que se pueden realizar es $(n-1)!$ porque tanto la primera como la última ciudad son la misma y son fijas. En esta implementación no existe un peor caso, ya que es necesario expandir todos los nodos en todos los casos, independientemente de si el mejor camino es el primero o el último.

  \vspace{2mm}

  En el código se puede ver que la función que lleva a cabo la búsqueda en profundidad está formada por un bucle \textit{while} que se ejecuta mientras la pila de nodos no visitados no esté vacía. En su interior hay 2 bucles \textit{for} anidados que se realizan ambos $n$ veces, el primero crea los hijos para cada nodo y el anidado actualiza la lista de nodos visitados y la lista con el orden nuevo. El segundo bucle se encuentra dentro de un \textit{if} cuya condición es verdadera tantas veces como ciudades queden por visitar.

  \vspace{2mm}

  Analizando el funcionamiento del código, se puede ver que el \textit{while}, junto con el primer \textit{for} se realiza tantas veces como nodos sean creados. El número de nodos creados depende del número de ciudades iniciales. Si se entienden los caminos como un árbol, en el primer nivel habría $n-1$ nodos, en el segundo $(n-1)*(n-2)$ nodos, etc. En la figura \ref{fig:ejemploCaminos} se ve con más detalle esta explicación para un ejemplo con 4 ciudades.

  \begin{figure}[!h]
    \imgcenter[95]{Images/grafoejemplo.png}
    \caption{Ejemplo de grafo de todos los caminos posibles con 4 ciudades sin regreso}
    \label{fig:ejemploCaminos}
  \end{figure}


  Por tanto el bucle \textit{while} junto con el primer \textit{for} se ejecuta:

  $$ \displaystyle\sum_{i=0}^{n-2} \frac{(n-1)!}{i!} $$

  veces, dado que no hay que tener en cuenta ni la raíz ni el regreso a la primera ciudad.

  \vspace{2mm}

  Así que la complejidad total de este algoritmo es:

  $$ T(n) = n * \displaystyle\sum_{i=0}^{n-2} \frac{(n-1)!}{i!} $$

  donde el primer $n$ corresponde al segundo \textit{for} y el sumatorio al \textit{while} junto al primer \textit{for} explicado previamente. Hay que multiplicar por $n$ dado que cada vez que se crea un nodo, este segundo \textit{for} tiene que inicializar los valores de ciudades ya visitadas y el orden por el que se visitan. Por tanto, la complejidad de este algoritmo en notación de \textit{Big-O} es:

  $$ O(n) = n! $$


  \subsubsection{Estudio Empírico}

  Para llevar a cabo el estudio empírico, se han utilizado varios ejemplos de grafos generados a partir de un \textit{script} generador de grafos, tanto simétricos como asimétricos. Este \textit{script} está hecho en \textit{Python}, toma como \textit{inputs} el número de ciudades y si el grafo debe ser o no simétrico, y devuelve un fichero con la matriz de costos para representar el grafo y el tamaño de dicha matriz (número de ciudades).

  \vspace{2mm}

  Se han creado diversos archivos, desde 3 hasta 19 ciudades. Para un grafo de $n$ ciudades, se han creado 10 grafos simétricos y 10 grafos asimétricos. Para cada ejemplo, se ha medido el tiempo de ejecución y se ha representado en la figura \ref{fig:compDFS}.

  \vspace{2mm}

  \begin{figure}[!h]
    \imgcenter[150]{Images/tiempo_comparativa_dfs_basico.png}
    \caption{Comparativa de tiempo DFS básico}
    \label{fig:compDFS}
  \end{figure}


  Los tiempos son prácticamente similares para mapas simétricos como no simétricos, aunque se puede apreciar un crecimiento muy rápido a partir de las 10 ciudades, como era esperable. El problema es que también se traduce en un crecimiento factorial del uso de memoria.

  \vspace{2mm}

  Para más de 12 ciudades el algoritmo no se puede ejecutar porque consume demasiada memoria y no es capaz de terminar la ejecución, por ello no aparecen sus valores en la gráfica.

  \subsubsection{Estudio Combinado}

  En este apartado se intentan aplicar la función de complejidad obtenida analíticamente junto a los datos empíricos. Se quiere demostrar si la complejidad que se obtiene al ejecutar el algoritmo coincide con el estudio analítico previo.

  \begin{figure}[!h]
    \imgcenter[150]{Images/dfs_combinado.png}
    \caption{Comparativa de valores empíricos y esperados del DFS básico}
    \label{fig:comp_combDFS}
  \end{figure}

  En la figura \ref{fig:comp_combDFS} se puede ver que la función se aplica con una precisión muy alta sobre los datos empíricos, confirmando que es posible que esta sea su complejidad. Sin embargo, al tener una cantidad tan reducida de ejemplos, y no poder probar mapas con mayor número de ciudades, no se puede asegurar que la complejidad sea la misma con ejemplos que hacen uso de una $n$ mayor.


  \subsection{Coste Computacional DFS con poda}

  \subsubsection{Estudio Analítico}

  Este acercamiento aplica el mismo algoritmo de búsqueda en profundidad iterativa, pero se le realiza una poda al árbol de búsqueda. Con esto se pretende reducir el número de nodos a expandir y por tanto que disminuya el tiempo y consumo de memoria requeridos. Asimismo se espera poder ejecutar mapas de mayor tamaño.

  \vspace{2mm}

  La poda consiste en no expandir nodos cuando el coste del camino acumulado supera el coste del mejor camino encontrado hasta el momento. Es una poda muy sencilla que permite reducir en gran medida el tiempo de computación. Sin embargo, al usar esta mejora aparece una diferenciación entre mejores y peores casos. El mejor caso se da cuando el primer camino es el óptimo y el resto de hijos de la raíz tienen un coste mayor que el camino entero. Por otro lado, el peor caso se da cuando cada camino es mejor que el anterior, de forma que es necesario expandir todos los nodos. Éste caso es idéntico, en complejidad, al algoritmo DFS sin poda.

  \vspace{2mm}

  Respecto a la implementación de este algoritmo, su código es prácticamente igual al anterior a excepción de que a la hora de crear nuevos hijos exite un \textit{if} que hace que solo se creen los hijos si el coste acumulado es menor que el mejor camino encontrado.

  \vspace{2mm}

  El estudio de la complejidad se divide en el estudio del mejor y peor caso por separado porque la complejidad de la mayoría de ejemplos se podrá ajustar usando la media de estos dos casos límite.

  \vspace{2mm}

  En el mejor de los casos será necesario expandir todo el primer nivel del árbol ($n-1$ nodos). Después hay que expandir todos los nodos de uno de ellos ($n-2$ nodos), y así sucesivamente se repite el proceso hasta que solo haya un único nodo. Entonces, la complejidad de este caso es:

  $$ T_{mejor}(n) = \displaystyle\sum_{i=1}^{n-1} n-i $$


  En el peor de los casos la complejidad será la misma que en el problema anterior, pues se tienen que expandir todos los nodos.

  $$ T_{peor}(n) = n * \displaystyle\sum_{i=0}^{n-2} \frac{(n-1)!}{i!} $$


  Por lo tanto, la complejidad media de este algoritmo estará entre la complejidad del peor caso y del mejor. Así que la complejidad del algoritmo DFS con poda en notación \textit{Big-O} sigue siendo:

  $$ O(n) = n! $$


  \subsubsection{Estudio Empírico}

  El estudio empírico se realiza con los mismos mapas simétricos y asimétricos que se crearon para el estudio del algoritmo sin poda. Los resultados obtenidos se muestran en la figura \ref{fig:compDFSpoda}, donde esta vez sí aparecen en la gráfica valores para mapas de mayor tamaño, llegando hasta mapas con 19 ciudades.

  \begin{figure}[!h]
    \imgcenter[150]{Images/tiempo_comparativa_dfs_poda.png}
    \caption{Comparativa de tiempo DFS con poda}
    \label{fig:compDFSpoda}
  \end{figure}

  Estudiando la gráfica de los resultados obtenidos al resolver el problema del viajante utilizando poda en el algoritmo, se observa que ahora, tanto los mapas simétricos como los mapas asimétricos, presentan diferencias en los tiempos de ejecución. Esto es debido a que en cada mapa se está aplicando una poda diferente y por tanto tienen un coste computacional distinto.

  \vspace{2mm}

  Además, conforme el tamaño de los mapas va aumentando, la diferencia de tiempo que se tarda en resolver mapas del mismo tamaño también va creciendo, pues cuantas más ciudades haya, más caminos hay, y más posibilidades de aplicar poda en la resolución del problema. También se observa que para tamaños grandes no se están ejecutando todos los mapas, algo que ya ocurría en el anterior algoritmo y que se debe a un gran consumo de memoria.




  \subsubsection{Estudio Combinado}

  Para el estudio combinado del algoritmo DFS con poda se quiere comprobar si las hipótesis del estudio analítico coinciden con los resultados obtenidos empíricamente. Se analizan por separado el peor y mejor caso con el fin de comprobar esta teoría. El problema surge debido a la enorme diferencia de tamaños entre el peor y el mejor caso. Mientras que el mejor caso tiene un coste polinómico, el peor tiene un coste factorial, que eclipsa por completo al otro caso a la hora de realizar comparaciones.

  \vspace{2mm}

  Graficar estas dos complejidades como límite inferior y superior es complejo dado estas diferencias. Por lo tanto se ha decidido no mostrar las gráficas dado que no aportaban nada al estudio. Se ha podido comprobar que los datos empíricos no siguen ni la función del mejor caso ni la del peor, sino una complejidad intermedia. Esto se debe a que los mapas graficados han sido generados aleatoriamente, y por tanto no corresponden ni a los peores ni a los mejores casos.





  \clearpage

  \section{Algoritmo de búsqueda local}

  Incialmente se realiza una búsqueda con un algoritmo \textit(greedy) o algoritmo voraz, que selecciona en cada paso el camino de menor coste, con la idea de conseguir la solución óptima. Sin embargo, los algoritmos voraces, a pesar de ser algoritmos completos, ya que garantizan encontrar siempre una solución, no garantizan que la solución encontrada sea óptima.

  \vspace{2mm}

  Una vez se tiene un camino usando la búsqueda \textit{greedy}, se aplica sobre éste una búsqueda local con 2-opt, que tratará de mejorar la solución ofrecida inicialmente. Para ello, el algoritmo se basa en cambiar el orden en el que se visitan algunas ciudades en la ruta ofrecida, recalculando el coste de recorrer todas las ciudades. En el caso de que el coste total de recorrer el grafo no mejore, el algoritmo se detiene, ofreciendo una solución a priori mejor que la solución incial. De nuevo, al igual que el algoritmo voraz, la optimización 2-opt no garantiza encontrar la solución óptima al problema.

  \vspace{2mm}

  En este caso, para la implementación del algoritmo no es necesaria una pila de nodos expandidos porque no hace falta explorar todos los caminos. Ya que se expande siempre en cada nodo el de menor coste.




  \subsection{Coste Computacional}

  \subsubsection{Estudio Analítico}

  Dado un número de ciudades $n$, el algoritmo voraz debe elegir una ciudad nueva únicamente cada vez que se desciende un nivel en el árbol. En el último nivel, todas las ciudades deben haber sido visitadas. Como todos los nodos del grafo se deben recorrer, no existe peor caso. En la figura \ref{fig:ejGreedy} se puede ver un ejemplo en un grafo de este algoritmo con 4 ciudades.

  \begin{figure}[!h]
    \imgcenter[20]{Images/caminoGreedy.png}
    \caption{Ejemplo de grafo con un único camino}
    \label{fig:ejGreedy}
  \end{figure}

  El algoritmo diseñado consta de un bucle \textit{for} exterior en el que se elige en cada iteración el nodo siguiente a crear, y un bucle \textit{for} interior, que elige el camino de menor coste para viajar a la siguiente ciudad, teniendo en cuenta no visitar ninguna ciudad que ya haya sido visitada. Por tanto, teniendo $n$ ciudades, el bucle exterior se ejecutará $n - 1$ veces, ya que la primera ciudad es la ciudad inicial, que se cuenta como visitada, y el bucle interior se ejecuta $n$ veces, puesto que en cada iteración se comprueba si cada ciudad ha sido visitada o no, incluyendo la ciudad inicial. La complejidad del algoritmo voraz queda por tanto como:

  $$T_{greedy}(n) = n*(n-1) = n^2 + n$$

  \vspace{2mm}

  La segunda parte de la solución planteada pasa por utilizar el algoritmo 2-opt para mejorar la solución dada por el algoritmo voraz. Este algoritmo elige dos ciudades de la ruta propuesta y cambia el orden de todas la ciudades que estén entre estas dos, con la intención de que, al intercambiar el orden, el coste total de recorrer todas las ciudades disminuya.

  \vspace{2mm}

  Teniendo en cuenta que cada una de las ciudades puede ocupar todas las posiciones en la ruta (excepto la ciudad de partida, que debe aparecer en la primera y última posición siempre), teniendo un total de $n$ ciudades, dado que la primera y última son fijas, tienen que poder combinarse $n-1$ ciudades.

  \vspace{2mm}

  La codificación del algoritmo consiste en ir realizando intercambios en el orden del camino base usando parejas de valores ($i$ y $k$), donde se realiza un bucle \textit{for} que itera por $i$ dándole un valor inicial de 1, y dentro hay un segundo bucle \textit{for} que itera por $k$ donde se inicializa siempre como $i+1$. Si se recorren ambos bucles realizando permutaciones sin haber encontrado una solución que mejore el estado actual, el algoritmo termina. En caso contrario, vuelve a realizar ambos bucles.

  \vspace{2mm}


   El peor caso se da cuando el algoritmo tiene que comprobar todas las posibles combinaciones. Por lo tanto tiene que realizar todas las permutaciones posibles sobre $n-1$ elementos, con una complejidad de:

  $$T_{2-opt}(n) = (n-1)!$$

  Por tanto, la complejidad total del algoritmo de búsqueda local es:

  $$T(n) = (n^2 + n) * (n-1)!$$

  Y se puede expresar en notación \textit{Big-O} como:

  $$O(n) = n!$$




  \subsubsection{Estudio Empírico}

  \begin{figure}[!h]
    \imgcenter[150]{Images/tiempo_comparativa_local.png}
    \caption{Comparativa de tiempo búsqueda local}
    \label{fig:compLocal}
  \end{figure}



  \subsubsection{Estudio Combinado}

  \clearpage

  \section{Conclusión}


\end{document}
